{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871ce426",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "410 Client Error: Gone for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m problem \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSophia finished 2/3 of a book. She finished 90 more pages than she has yet to read. How long is the book?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA) 229, B) 270, C) 877, D) 266, E) 281\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mpredict_answer\u001b[1;34m(problem, options)\u001b[0m\n\u001b[0;32m     30\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     }\n\u001b[0;32m     37\u001b[0m }\n\u001b[0;32m     39\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(API_URL, headers\u001b[38;5;241m=\u001b[39mHEADERS, json\u001b[38;5;241m=\u001b[39mpayload, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m out \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# HF Inference API commonly returns: [{\"generated_text\": \"...\"}]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 410 Client Error: Gone for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")  # set env var HF_TOKEN=...\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "def build_prompt(problem: str, options: str) -> str:\n",
    "    return f\"\"\"You are solving a multiple-choice math word problem.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Return ONLY the letter of the correct option (A, B, C, D, or E).\"\"\"\n",
    "\n",
    "def extract_choice(text: str) -> str:\n",
    "    # Accept outputs like \"A\", \"Answer: B\", \"(c)\", etc.\n",
    "    m = re.search(r\"\\b([ABCDE])\\b\", text.strip().upper())\n",
    "    return m.group(1) if m else \"E\"\n",
    "\n",
    "def predict_answer(problem: str, options: str) -> dict:\n",
    "    prompt = build_prompt(problem, options)\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 5,\n",
    "            \"temperature\": 0.0,\n",
    "            \"return_full_text\": False,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    r = requests.post(API_URL, headers=HEADERS, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out = r.json()\n",
    "\n",
    "    # HF Inference API commonly returns: [{\"generated_text\": \"...\"}]\n",
    "    if isinstance(out, list) and out and \"generated_text\" in out[0]:\n",
    "        generated = out[0][\"generated_text\"]\n",
    "    else:\n",
    "        # Some backends return different shapes\n",
    "        generated = str(out)\n",
    "\n",
    "    choice = extract_choice(generated)\n",
    "    return {\"choice\": choice, \"raw_generation\": generated}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    problem = \"Sophia finished 2/3 of a book. She finished 90 more pages than she has yet to read. How long is the book?\"\n",
    "    options = \"A) 229, B) 270, C) 877, D) 266, E) 281\"\n",
    "    print(predict_answer(problem, options))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec71b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
